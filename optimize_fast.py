#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Diabetes Prediction System - FAST Model Optimization
Huáº¥n luyá»‡n nhanh vá»›i hyperparameter tuning
Má»¥c tiÃªu: Äáº¡t Accuracy >= 70%
"""

import os
import sys
import pandas as pd
import numpy as np
import joblib
import warnings
from datetime import datetime

warnings.filterwarnings('ignore')

print("\n" + "="*80)
print("  DIABETES PREDICTION - FAST MODEL OPTIMIZATION")
print("  Má»¥c tiÃªu: Äáº¡t Accuracy >= 70%")
print("="*80 + "\n")

# ============================================================================
# 1. Láº¤Y Dá»® LIá»†U
# ============================================================================
print("ğŸ“Š BÆ¯á»šC 1: CHUáº¨N Bá»Š Dá»® LIá»†U")
print("-" * 80)

from src.preprocessing import DiabetesDataPreprocessor

preprocessor = DiabetesDataPreprocessor(random_state=42)
df = preprocessor.load_data('data/diabetes.csv')

print(f"âœ“ Dataset: {df.shape[0]} máº«u Ã— {df.shape[1]} cá»™t")
print(f"  Outcome: {(df['Outcome']==1).sum()} positive, {(df['Outcome']==0).sum()} negative")

df_clean = preprocessor.handle_missing_values(df)
X_train, X_test, y_train, y_test = preprocessor.prepare_data(df_clean, test_size=0.2)
feature_names = preprocessor.get_feature_names()
scaler = preprocessor.scaler

print(f"\nâœ“ Train: {X_train.shape[0]} máº«u | Test: {X_test.shape[0]} máº«u")

# ============================================================================
# 2. HUáº¤N LUYá»†N NHANH
# ============================================================================
print("\nğŸ“ˆ BÆ¯á»šC 2: HUáº¤N LUYá»†N NHANH")
print("-" * 80)

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.neighbors import KNeighborsClassifier
import xgboost as xgb
from sklearn.model_selection import cross_val_score
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

models = {}
results = {}

# ============================================================================
# 2.1 BEST KNN (thá»­ k khÃ¡c nhau)
# ============================================================================
print("\nğŸ”¹ 1. K-NEAREST NEIGHBORS (tá»‘i Æ°u k)")

best_knn = None
best_knn_score = 0

for k in [3, 5, 7, 9, 11, 13]:
    knn = KNeighborsClassifier(n_neighbors=k, weights='distance', n_jobs=-1)
    score = cross_val_score(knn, X_train, y_train, cv=5, scoring='accuracy').mean()
    
    if score > best_knn_score:
        best_knn_score = score
        best_knn = knn

best_knn.fit(X_train, y_train)
knn_pred = best_knn.predict(X_test)
knn_acc = accuracy_score(y_test, knn_pred)
knn_auc = roc_auc_score(y_test, best_knn.predict_proba(X_test)[:, 1])

print(f"  âœ“ Best k: {best_knn.n_neighbors}")
print(f"  âœ“ Test Accuracy: {knn_acc:.4f}")
print(f"  âœ“ ROC-AUC: {knn_auc:.4f}")

models['KNN'] = best_knn
results['KNN'] = knn_acc

# ============================================================================
# 2.2 BEST RANDOM FOREST
# ============================================================================
print("\nğŸ”¹ 2. RANDOM FOREST (tá»‘i Æ°u)")

best_rf = None
best_rf_score = 0

for n_est in [100, 150, 200]:
    for max_d in [10, 15, 20]:
        rf = RandomForestClassifier(n_estimators=n_est, max_depth=max_d, 
                                    random_state=42, n_jobs=-1)
        score = cross_val_score(rf, X_train, y_train, cv=5, scoring='accuracy').mean()
        
        if score > best_rf_score:
            best_rf_score = score
            best_rf = rf

best_rf.fit(X_train, y_train)
rf_pred = best_rf.predict(X_test)
rf_acc = accuracy_score(y_test, rf_pred)
rf_auc = roc_auc_score(y_test, best_rf.predict_proba(X_test)[:, 1])

print(f"  âœ“ n_estimators: {best_rf.n_estimators}, max_depth: {best_rf.max_depth}")
print(f"  âœ“ Test Accuracy: {rf_acc:.4f}")
print(f"  âœ“ ROC-AUC: {rf_auc:.4f}")

models['Random Forest'] = best_rf
results['Random Forest'] = rf_acc

# ============================================================================
# 2.3 BEST XGBOOST
# ============================================================================
print("\nğŸ”¹ 3. XGBOOST (tá»‘i Æ°u)")

best_xgb = None
best_xgb_score = 0

for n_est in [100, 150]:
    for max_d in [5, 7]:
        for lr in [0.05, 0.1]:
            xgb_model = xgb.XGBClassifier(n_estimators=n_est, max_depth=max_d, 
                                         learning_rate=lr, random_state=42, 
                                         eval_metric='logloss', verbosity=0)
            score = cross_val_score(xgb_model, X_train, y_train, cv=5, scoring='accuracy').mean()
            
            if score > best_xgb_score:
                best_xgb_score = score
                best_xgb = xgb_model

best_xgb.fit(X_train, y_train)
xgb_pred = best_xgb.predict(X_test)
xgb_acc = accuracy_score(y_test, xgb_pred)
xgb_auc = roc_auc_score(y_test, best_xgb.predict_proba(X_test)[:, 1])

print(f"  âœ“ n_estimators: {best_xgb.n_estimators}, max_depth: {best_xgb.max_depth}, lr: {best_xgb.learning_rate}")
print(f"  âœ“ Test Accuracy: {xgb_acc:.4f}")
print(f"  âœ“ ROC-AUC: {xgb_auc:.4f}")

models['XGBoost'] = best_xgb
results['XGBoost'] = xgb_acc

# ============================================================================
# 2.4 GRADIENT BOOSTING
# ============================================================================
print("\nğŸ”¹ 4. GRADIENT BOOSTING (tá»‘i Æ°u)")

best_gb = None
best_gb_score = 0

for n_est in [100, 150]:
    for lr in [0.05, 0.1]:
        gb = GradientBoostingClassifier(n_estimators=n_est, learning_rate=lr, 
                                        max_depth=5, random_state=42)
        score = cross_val_score(gb, X_train, y_train, cv=5, scoring='accuracy').mean()
        
        if score > best_gb_score:
            best_gb_score = score
            best_gb = gb

best_gb.fit(X_train, y_train)
gb_pred = best_gb.predict(X_test)
gb_acc = accuracy_score(y_test, gb_pred)
gb_auc = roc_auc_score(y_test, best_gb.predict_proba(X_test)[:, 1])

print(f"  âœ“ n_estimators: {best_gb.n_estimators}, learning_rate: {best_gb.learning_rate}")
print(f"  âœ“ Test Accuracy: {gb_acc:.4f}")
print(f"  âœ“ ROC-AUC: {gb_auc:.4f}")

models['Gradient Boosting'] = best_gb
results['Gradient Boosting'] = gb_acc

# ============================================================================
# 2.5 LOGISTIC REGRESSION
# ============================================================================
print("\nğŸ”¹ 5. LOGISTIC REGRESSION")

lr = LogisticRegression(C=1, max_iter=1000, random_state=42)
lr.fit(X_train, y_train)
lr_pred = lr.predict(X_test)
lr_acc = accuracy_score(y_test, lr_pred)
lr_auc = roc_auc_score(y_test, lr.predict_proba(X_test)[:, 1])

print(f"  âœ“ Test Accuracy: {lr_acc:.4f}")
print(f"  âœ“ ROC-AUC: {lr_auc:.4f}")

models['Logistic Regression'] = lr
results['Logistic Regression'] = lr_acc

# ============================================================================
# 3. Káº¾T QUáº¢
# ============================================================================
print("\n\n" + "="*80)
print("ğŸ“Š Káº¾T QUáº¢ CUá»I CÃ™NG")
print("="*80)

sorted_results = sorted(results.items(), key=lambda x: x[1], reverse=True)

print("\nXáº¿p háº¡ng mÃ´ hÃ¬nh (Test Accuracy):\n")
for i, (model_name, accuracy) in enumerate(sorted_results, 1):
    status = "âœ…" if accuracy >= 0.70 else "âš ï¸ "
    print(f"  {i}. {status} {model_name:<25} {accuracy:.2%}")

# ============================================================================
# 4. LÆ¯U MÃ” HÃŒNH
# ============================================================================
print("\n" + "-"*80)
print("ğŸ’¾ LÆ¯U MÃ” HÃŒNH")
print("-"*80)

os.makedirs('models', exist_ok=True)

for model_name, model in models.items():
    model_file = f"models/{model_name.lower().replace(' ', '_')}_optimized.pkl"
    joblib.dump(model, model_file)
    print(f"  âœ“ {model_name}: {model_file}")

joblib.dump(scaler, 'models/scaler.pkl')
joblib.dump(feature_names, 'models/feature_names.pkl')
print(f"  âœ“ Scaler: models/scaler.pkl")
print(f"  âœ“ Feature names: models/feature_names.pkl")

# ============================================================================
# 5. CHI TIáº¾T
# ============================================================================
print("\n" + "="*80)
print("ğŸ“ˆ CHI TIáº¾T Káº¾T QUáº¢")
print("="*80 + "\n")

print(f"{'Model':<25} {'Accuracy':<12} {'Precision':<12} {'Recall':<12} {'F1-Score':<12} {'ROC-AUC':<12}")
print("-" * 85)

for model_name, model in models.items():
    y_pred = model.predict(X_test)
    y_pred_proba = model.predict_proba(X_test)[:, 1]
    
    acc = accuracy_score(y_test, y_pred)
    prec = precision_score(y_test, y_pred, zero_division=0)
    rec = recall_score(y_test, y_pred, zero_division=0)
    f1 = f1_score(y_test, y_pred, zero_division=0)
    roc_auc = roc_auc_score(y_test, y_pred_proba)
    
    print(f"{model_name:<25} {acc:<12.4f} {prec:<12.4f} {rec:<12.4f} {f1:<12.4f} {roc_auc:<12.4f}")

# ============================================================================
# 6. Káº¾T LUáº¬N
# ============================================================================
print("\n" + "="*80)
print("âœ… KIá»‚M Äá»ŠNH Káº¾T QUáº¢")
print("="*80)

best_model_name = sorted_results[0][0]
best_accuracy = sorted_results[0][1]

print(f"\nğŸ† MÃ´ hÃ¬nh tá»‘t nháº¥t: {best_model_name}")
print(f"   Test Accuracy: {best_accuracy:.2%}")

if best_accuracy >= 0.70:
    print(f"\n   âœ… Äáº T Má»¤C TIÃŠU >= 70%!")
else:
    print(f"\n   âš ï¸  Káº¿t quáº£ tháº¥p hÆ¡n 70%, nhÆ°ng Ä‘Ã£ tá»‘i Æ°u tá»‘t nháº¥t!")

print("\nğŸ’¡ LÆ°u Ã½:")
print("   - Dá»¯ liá»‡u hiá»‡n táº¡i chá»‰ cÃ³ 100 máº«u (test set)")
print("   - Accuracy sáº½ cao hÆ¡n khi sá»­ dá»¥ng toÃ n bá»™ Kaggle dataset (768 máº«u)")
print("   - Äá»ƒ Ä‘áº¡t 70%+ trÃªn dataset nhá», cáº§n thÃªm feature engineering")

print("\n" + "="*80)
print(f"HoÃ n táº¥t lÃºc: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
print("="*80 + "\n")
