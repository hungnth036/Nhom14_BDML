#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Diabetes Prediction System - Random Forest Optimization
T·ªëi ∆∞u h√≥a Random Forest ƒë·ªÉ ƒë·∫°t >70% Accuracy
"""

import os
import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings('ignore')

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, confusion_matrix, classification_report
)
from sklearn.utils.class_weight import compute_class_weight
import joblib

print("\n" + "="*80)
print("  RANDOM FOREST OPTIMIZATION - T·ªëi ∆∞u h√≥a Random Forest")
print("="*80 + "\n")

# 1. Load Data
print("üìä B∆∞·ªõc 1: T·∫£i d·ªØ li·ªáu...")
try:
    df = pd.read_csv('data/diabetes.csv')
    print(f"  ‚úì T·∫£i th√†nh c√¥ng: {df.shape[0]} m·∫´u √ó {df.shape[1]} c·ªôt")
except Exception as e:
    print(f"  ‚úó L·ªói: {e}")
    exit(1)

# 2. Data Preprocessing
print("\n‚öôÔ∏è  B∆∞·ªõc 2: X·ª≠ l√Ω d·ªØ li·ªáu...")

# Handle missing values (zeros)
df_clean = df.copy()
cols_with_zero = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']
for col in cols_with_zero:
    zero_count = (df_clean[col] == 0).sum()
    if zero_count > 0:
        median_val = df_clean[df_clean[col] != 0][col].median()
        df_clean.loc[df_clean[col] == 0, col] = median_val
        print(f"  ‚úì {col}: thay {zero_count} gi√° tr·ªã 0 b·∫±ng {median_val:.2f}")

# Separate features and target
X = df_clean.drop('Outcome', axis=1)
y = df_clean['Outcome']

# Split train/test
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Normalize features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print(f"  ‚úì Train: {X_train.shape[0]} m·∫´u, Test: {X_test.shape[0]} m·∫´u")
print(f"  ‚úì Train - Class 0: {(y_train==0).sum()}, Class 1: {(y_train==1).sum()}")
print(f"  ‚úì Test  - Class 0: {(y_test==0).sum()}, Class 1: {(y_test==1).sum()}")

# 3. Calculate Class Weights (ƒë·ªÉ c√¢n b·∫±ng dataset)
print("\n‚öñÔ∏è  B∆∞·ªõc 3: C√¢n b·∫±ng d·ªØ li·ªáu...")
class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)
class_weight_dict = {0: class_weights[0], 1: class_weights[1]}
print(f"  ‚úì Class weight 0: {class_weight_dict[0]:.2f}")
print(f"  ‚úì Class weight 1: {class_weight_dict[1]:.2f}")

# 4. Hyperparameter Tuning with GridSearchCV
print("\nüîß B∆∞·ªõc 4: T·ªëi ∆∞u hyperparameters (GridSearchCV)...")
print("  ‚è≥ ƒêang t√¨m ki·∫øm tham s·ªë t·ªëi ∆∞u...")

param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [5, 10, 15, 20],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'max_features': ['sqrt', 'log2'],
}

rf_base = RandomForestClassifier(
    random_state=42,
    n_jobs=-1,
    class_weight='balanced'
)

grid_search = GridSearchCV(
    rf_base,
    param_grid,
    cv=5,
    scoring='f1',
    n_jobs=-1,
    verbose=0
)

grid_search.fit(X_train_scaled, y_train)

print(f"\n  ‚úì T√¨m ki·∫øm ho√†n t·∫•t!")
print(f"  ‚úì Best F1-Score (CV): {grid_search.best_score_:.4f}")
print(f"  ‚úì Best Parameters:")
for param, value in grid_search.best_params_.items():
    print(f"     - {param}: {value}")

# 5. Train Best Model
print("\nü§ñ B∆∞·ªõc 5: Hu·∫•n luy·ªán m√¥ h√¨nh t·ªëi ∆∞u...")
best_rf = grid_search.best_estimator_

# Cross-validation on best model
cv_scores = cross_val_score(best_rf, X_train_scaled, y_train, cv=5, scoring='f1')
print(f"  ‚úì 5-Fold CV F1-Score: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})")

# 6. Evaluate on Test Set
print("\nüìà B∆∞·ªõc 6: ƒê√°nh gi√° tr√™n t·∫≠p test...")

y_pred = best_rf.predict(X_test_scaled)
y_pred_proba = best_rf.predict_proba(X_test_scaled)[:, 1]

accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, y_pred_proba)

print(f"  ‚úì Accuracy:  {accuracy:.4f} ({accuracy*100:.1f}%)")
print(f"  ‚úì Precision: {precision:.4f}")
print(f"  ‚úì Recall:    {recall:.4f}")
print(f"  ‚úì F1-Score:  {f1:.4f}")
print(f"  ‚úì ROC-AUC:   {roc_auc:.4f}")

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
print(f"\n  Confusion Matrix:")
print(f"    TN: {cm[0,0]:<5} FP: {cm[0,1]:<5}")
print(f"    FN: {cm[1,0]:<5} TP: {cm[1,1]:<5}")

# Classification Report
print(f"\n  Classification Report:")
print(classification_report(y_test, y_pred, target_names=['No Diabetes', 'Diabetes']))

# 7. Feature Importance
print("\nüîç B∆∞·ªõc 7: T·∫ßm quan tr·ªçng c·ªßa c√°c ƒë·∫∑c tr∆∞ng...")
feature_importance = pd.DataFrame({
    'Feature': X.columns,
    'Importance': best_rf.feature_importances_
}).sort_values('Importance', ascending=False)

for idx, row in feature_importance.head(5).iterrows():
    print(f"  {row['Feature']:<30} {row['Importance']:.4f}")

# 8. Save Model
print("\nüíæ B∆∞·ªõc 8: L∆∞u m√¥ h√¨nh t·ªëi ∆∞u...")
os.makedirs('models', exist_ok=True)

joblib.dump(best_rf, 'models/random_forest_optimized.pkl')
joblib.dump(scaler, 'models/scaler_optimized.pkl')
joblib.dump(X.columns.tolist(), 'models/feature_names_optimized.pkl')

print(f"  ‚úì models/random_forest_optimized.pkl")
print(f"  ‚úì models/scaler_optimized.pkl")
print(f"  ‚úì models/feature_names_optimized.pkl")

# 9. Summary
print("\n" + "="*80)
if accuracy >= 0.70:
    print(f"  ‚úÖ TH√ÄNH C√îNG! Accuracy: {accuracy*100:.1f}% (>= 70%)")
else:
    print(f"  ‚ö†Ô∏è  Accuracy: {accuracy*100:.1f}% (< 70%)")
print("="*80 + "\n")

# 10. Save Results
results = {
    'Best Parameters': grid_search.best_params_,
    'Test Metrics': {
        'Accuracy': accuracy,
        'Precision': precision,
        'Recall': recall,
        'F1-Score': f1,
        'ROC-AUC': roc_auc,
    },
    'CV Score': cv_scores.mean(),
    'Feature Importance': feature_importance.to_dict()
}

import json
with open('results/random_forest_optimization_results.json', 'w') as f:
    # Convert numpy types to Python types for JSON serialization
    def convert(o):
        if isinstance(o, np.integer):
            return int(o)
        elif isinstance(o, np.floating):
            return float(o)
        elif isinstance(o, np.ndarray):
            return o.tolist()
        raise TypeError

    json.dump(results, f, indent=2, default=convert)
    print("‚úì K·∫øt qu·∫£ l∆∞u v√†o: results/random_forest_optimization_results.json\n")
