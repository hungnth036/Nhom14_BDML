{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b27365",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ H·ª¶A LUY·ªÜN V√Ä L∆ØU M√î H√åNH HO√ÄN TH√ÄNH\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nüìù C√°c b∆∞·ªõc ti·∫øp theo:\")\n",
    "print(\"1. ‚úì Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu\")\n",
    "print(\"2. ‚úì Hu·∫•n luy·ªán c√°c m√¥ h√¨nh\")\n",
    "print(\"3. ‚Üí ƒê√°nh gi√° m√¥ h√¨nh (Evaluation)\")\n",
    "print(\"4. ‚Üí Ph√¢n t√≠ch Feature Importance\")\n",
    "print(\"5. ‚Üí D·ª± ƒëo√°n nguy c∆° ti·ªÉu ƒë∆∞·ªùng (Demo)\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb96d8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import os\n",
    "\n",
    "# T·∫°o th∆∞ m·ª•c models n·∫øu ch∆∞a t·ªìn t·∫°i\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "\n",
    "# L∆∞u m√¥ h√¨nh\n",
    "for model_name, model in models.items():\n",
    "    model_path = f'../models/{model_name.replace(\" \", \"_\").lower()}_model.pkl'\n",
    "    joblib.dump(model, model_path)\n",
    "    print(f\"‚úì L∆∞u m√¥ h√¨nh: {model_path}\")\n",
    "\n",
    "# L∆∞u scaler\n",
    "scaler_path = '../models/scaler.pkl'\n",
    "joblib.dump(scaler, scaler_path)\n",
    "print(f\"‚úì L∆∞u scaler: {scaler_path}\")\n",
    "\n",
    "# L∆∞u feature names\n",
    "features_path = '../models/feature_names.pkl'\n",
    "joblib.dump(feature_names, features_path)\n",
    "print(f\"‚úì L∆∞u feature names: {features_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb94e94",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ L∆∞u M√¥ H√¨nh v√† Scaler\n",
    "\n",
    "C√°c m√¥ h√¨nh s·∫Ω ƒë∆∞·ª£c s·ª≠ d·ª•ng trong ph·∫ßn ƒë√°nh gi√° ti·∫øp theo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38fb459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# V·∫Ω bi·ªÉu ƒë·ªì so s√°nh CV scores\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "model_names = list(models.keys())\n",
    "cv_means = [lr_cv_scores.mean(), rf_cv_scores.mean(), xgb_cv_scores.mean(), knn_cv_scores.mean()]\n",
    "cv_stds = [lr_cv_scores.std(), rf_cv_scores.std(), xgb_cv_scores.std(), knn_cv_scores.std()]\n",
    "\n",
    "colors = ['#3498db', '#2ecc71', '#e74c3c', '#f39c12']\n",
    "bars = ax.bar(model_names, cv_means, yerr=cv_stds, capsize=5, color=colors, alpha=0.8, edgecolor='black', linewidth=1.5)\n",
    "\n",
    "ax.set_ylabel('Cross-validation F1-Score', fontweight='bold', fontsize=11)\n",
    "ax.set_title('üìä So S√°nh Cross-Validation F1-Scores C√°c M√¥ H√¨nh', fontweight='bold', fontsize=13)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Th√™m gi√° tr·ªã tr√™n c·ªôt\n",
    "for bar, mean in zip(bars, cv_means):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{mean:.3f}', ha='center', va='bottom', fontweight='bold', fontsize=10)\n",
    "\n",
    "plt.xticks(rotation=15, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Bi·ªÉu ƒë·ªì so s√°nh ƒë√£ ƒë∆∞·ª£c v·∫Ω\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2b63c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T√≥m t·∫Øt CV scores\n",
    "cv_summary = {\n",
    "    'Model': ['Logistic Regression', 'Random Forest', 'XGBoost', 'KNN'],\n",
    "    'CV F1-Score': [\n",
    "        f\"{lr_cv_scores.mean():.4f}\",\n",
    "        f\"{rf_cv_scores.mean():.4f}\",\n",
    "        f\"{xgb_cv_scores.mean():.4f}\",\n",
    "        f\"{knn_cv_scores.mean():.4f}\"\n",
    "    ],\n",
    "    'Std': [\n",
    "        f\"¬±{lr_cv_scores.std():.4f}\",\n",
    "        f\"¬±{rf_cv_scores.std():.4f}\",\n",
    "        f\"¬±{xgb_cv_scores.std():.4f}\",\n",
    "        f\"¬±{knn_cv_scores.std():.4f}\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "cv_df = pd.DataFrame(cv_summary)\n",
    "print(\"\\nüìä T√≥m t·∫Øt Cross-Validation F1-Scores:\")\n",
    "print(cv_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee80a8b",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ So S√°nh Cross-Validation Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d5c64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ H·ª¶A LUY·ªÜN T·∫§T C·∫¢ M√î H√åNH HO√ÄN TH√ÄNH\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nC√°c m√¥ h√¨nh ƒë√£ hu·∫•n luy·ªán: {list(models.keys())}\")\n",
    "print(f\"S·ªë l∆∞·ª£ng m√¥ h√¨nh: {len(models)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b612e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. KNN\n",
    "print(\"\\nüîπ Hu·∫•n luy·ªán KNN...\")\n",
    "print(\"=\"*60)\n",
    "knn_model = KNeighborsClassifier(n_neighbors=5, n_jobs=-1)\n",
    "knn_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Cross-validation\n",
    "knn_cv_scores = cross_val_score(knn_model, X_train_scaled, y_train, cv=5, scoring='f1')\n",
    "print(f\"  ‚úì Training ho√†n t·∫•t\")\n",
    "print(f\"  ‚úì Cross-validation F1-Score: {knn_cv_scores.mean():.4f} (+/- {knn_cv_scores.std():.4f})\")\n",
    "\n",
    "models['KNN'] = knn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c5abf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. XGBoost\n",
    "print(\"\\nüîπ Hu·∫•n luy·ªán XGBoost...\")\n",
    "print(\"=\"*60)\n",
    "xgb_model = xgb.XGBClassifier(n_estimators=100, random_state=42, eval_metric='logloss', verbosity=0)\n",
    "xgb_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Cross-validation\n",
    "xgb_cv_scores = cross_val_score(xgb_model, X_train_scaled, y_train, cv=5, scoring='f1')\n",
    "print(f\"  ‚úì Training ho√†n t·∫•t\")\n",
    "print(f\"  ‚úì Cross-validation F1-Score: {xgb_cv_scores.mean():.4f} (+/- {xgb_cv_scores.std():.4f})\")\n",
    "\n",
    "models['XGBoost'] = xgb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88475fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Random Forest\n",
    "print(\"\\nüîπ Hu·∫•n luy·ªán Random Forest...\")\n",
    "print(\"=\"*60)\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Cross-validation\n",
    "rf_cv_scores = cross_val_score(rf_model, X_train_scaled, y_train, cv=5, scoring='f1')\n",
    "print(f\"  ‚úì Training ho√†n t·∫•t\")\n",
    "print(f\"  ‚úì Cross-validation F1-Score: {rf_cv_scores.mean():.4f} (+/- {rf_cv_scores.std():.4f})\")\n",
    "\n",
    "models['Random Forest'] = rf_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fe85d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}\n",
    "\n",
    "# 1. Logistic Regression\n",
    "print(\"\\nüîπ Hu·∫•n luy·ªán Logistic Regression...\")\n",
    "print(\"=\"*60)\n",
    "lr_model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Cross-validation\n",
    "lr_cv_scores = cross_val_score(lr_model, X_train_scaled, y_train, cv=5, scoring='f1')\n",
    "print(f\"  ‚úì Training ho√†n t·∫•t\")\n",
    "print(f\"  ‚úì Cross-validation F1-Score: {lr_cv_scores.mean():.4f} (+/- {lr_cv_scores.std():.4f})\")\n",
    "\n",
    "models['Logistic Regression'] = lr_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548d91e1",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Hu·∫•n Luy·ªán C√°c M√¥ H√¨nh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a31a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chu·∫©n h√≥a d·ªØ li·ªáu\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"\\n‚úì Chu·∫©n h√≥a d·ªØ li·ªáu (StandardScaler):\")\n",
    "print(f\"  - Mean (train): {X_train_scaled.mean(axis=0).round(2)}\")\n",
    "print(f\"  - Std (train): {X_train_scaled.std(axis=0).round(2)}\")\n",
    "\n",
    "# Chuy·ªÉn th√†nh DataFrame cho ti·ªán s·ª≠ d·ª•ng\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=feature_names)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d004ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T√°ch X, y\n",
    "X = df_clean.drop('Outcome', axis=1)\n",
    "y = df_clean['Outcome']\n",
    "\n",
    "feature_names = X.columns.tolist()\n",
    "\n",
    "print(f\"\\n‚úì T√°ch Features v√† Target:\")\n",
    "print(f\"  - Features: {feature_names}\")\n",
    "print(f\"  - Target: Outcome\")\n",
    "\n",
    "# Split train/test (80/20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úì Chia train/test (80/20):\")\n",
    "print(f\"  - Train: {X_train.shape[0]} m·∫´u\")\n",
    "print(f\"  - Test: {X_test.shape[0]} m·∫´u\")\n",
    "print(f\"  - Train - Outcome=0: {(y_train==0).sum()}, Outcome=1: {(y_train==1).sum()}\")\n",
    "print(f\"  - Test  - Outcome=0: {(y_test==0).sum()}, Outcome=1: {(y_test==1).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001552f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X·ª≠ l√Ω gi√° tr·ªã 0 kh√¥ng h·ª£p l·ªá\n",
    "print(\"\\nüìä X·ª≠ l√Ω gi√° tr·ªã 0 kh√¥ng h·ª£p l·ªá:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "df_clean = df.copy()\n",
    "\n",
    "# Danh s√°ch c·ªôt kh√¥ng n√™n c√≥ gi√° tr·ªã 0\n",
    "cols_with_zero = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']\n",
    "\n",
    "for col in cols_with_zero:\n",
    "    zero_count = (df_clean[col] == 0).sum()\n",
    "    if zero_count > 0:\n",
    "        # T√≠nh median c·ªßa nh·ªØng gi√° tr·ªã kh√°c 0\n",
    "        median_val = df_clean[df_clean[col] != 0][col].median()\n",
    "        df_clean.loc[df_clean[col] == 0, col] = median_val\n",
    "        print(f\"  {col}: Thay {zero_count} gi√° tr·ªã 0 b·∫±ng median ({median_val:.2f})\")\n",
    "\n",
    "print(\"\\n‚úì X·ª≠ l√Ω gi√° tr·ªã 0 ho√†n t·∫•t!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88802c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T·∫£i d·ªØ li·ªáu\n",
    "data_path = '../data/diabetes.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "print(f\"‚úì T·∫£i d·ªØ li·ªáu th√†nh c√¥ng: {df.shape[0]} m·∫´u, {df.shape[1]} c·ªôt\")\n",
    "print(f\"\\nT√™n c√°c c·ªôt: {df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee22bba",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ T·∫£i v√† Ti·ªÅn X·ª≠ L√Ω D·ªØ Li·ªáu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34e581d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úì C√°c th∆∞ vi·ªán ƒë√£ ƒë∆∞·ª£c import th√†nh c√¥ng!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356ca09f",
   "metadata": {},
   "source": [
    "# ü§ñ D·ª± ƒëo√°n B·ªánh Ti·ªÉu ƒê∆∞·ªùng - Hu·∫•n Luy·ªán M√¥ H√¨nh\n",
    "\n",
    "## Ph·∫ßn 2: Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu & Hu·∫•n luy·ªán c√°c m√¥ h√¨nh ML\n",
    "- Logistic Regression\n",
    "- Random Forest\n",
    "- XGBoost\n",
    "- KNN"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
