"""
Model Training Module
Hu·∫•n luy·ªán c√°c m√¥ h√¨nh: Logistic Regression, Random Forest, XGBoost
"""

import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
import xgboost as xgb
from sklearn.model_selection import cross_val_score


class DiabetesModelTrainer:
    """Hu·∫•n luy·ªán c√°c m√¥ h√¨nh d·ª± ƒëo√°n"""
    
    def __init__(self, random_state=42):
        self.random_state = random_state
        self.models = {}
        self.best_model = None
        self.best_model_name = None
        
    def train_logistic_regression(self, X_train, y_train):
        """Hu·∫•n luy·ªán Logistic Regression"""
        print("\nüîπ Hu·∫•n luy·ªán Logistic Regression...")
        model = LogisticRegression(random_state=self.random_state, max_iter=1000)
        model.fit(X_train, y_train)
        
        # Cross-validation
        cv_score = cross_val_score(model, X_train, y_train, cv=5, scoring='f1')
        print(f"   ‚úì Cross-validation F1-Score: {cv_score.mean():.4f} (+/- {cv_score.std():.4f})")
        
        self.models['Logistic Regression'] = model
        return model
    
    def train_random_forest(self, X_train, y_train, n_estimators=100):
        """Hu·∫•n luy·ªán Random Forest"""
        print("\nüîπ Hu·∫•n luy·ªán Random Forest...")
        model = RandomForestClassifier(
            n_estimators=n_estimators,
            random_state=self.random_state,
            n_jobs=-1
        )
        model.fit(X_train, y_train)
        
        # Cross-validation
        cv_score = cross_val_score(model, X_train, y_train, cv=5, scoring='f1')
        print(f"   ‚úì Cross-validation F1-Score: {cv_score.mean():.4f} (+/- {cv_score.std():.4f})")
        
        self.models['Random Forest'] = model
        return model
    
    def train_xgboost(self, X_train, y_train, n_estimators=100):
        """Hu·∫•n luy·ªán XGBoost"""
        print("\nüîπ Hu·∫•n luy·ªán XGBoost...")
        model = xgb.XGBClassifier(
            n_estimators=n_estimators,
            random_state=self.random_state,
            eval_metric='logloss',
            verbosity=0
        )
        model.fit(X_train, y_train)
        
        # Cross-validation
        cv_score = cross_val_score(model, X_train, y_train, cv=5, scoring='f1')
        print(f"   ‚úì Cross-validation F1-Score: {cv_score.mean():.4f} (+/- {cv_score.std():.4f})")
        
        self.models['XGBoost'] = model
        return model
    
    def train_knn(self, X_train, y_train, n_neighbors=5):
        """Hu·∫•n luy·ªán KNN"""
        print("\nüîπ Hu·∫•n luy·ªán KNN...")
        model = KNeighborsClassifier(n_neighbors=n_neighbors, n_jobs=-1)
        model.fit(X_train, y_train)
        
        # Cross-validation
        cv_score = cross_val_score(model, X_train, y_train, cv=5, scoring='f1')
        print(f"   ‚úì Cross-validation F1-Score: {cv_score.mean():.4f} (+/- {cv_score.std():.4f})")
        
        self.models['KNN'] = model
        return model
    
    def get_model(self, model_name):
        """L·∫•y m√¥ h√¨nh theo t√™n"""
        return self.models.get(model_name)
    
    def get_all_models(self):
        """L·∫•y t·∫•t c·∫£ m√¥ h√¨nh ƒë√£ hu·∫•n luy·ªán"""
        return self.models
    
    def set_best_model(self, model_name):
        """ƒê·∫∑t m√¥ h√¨nh t·ªët nh·∫•t"""
        if model_name in self.models:
            self.best_model = self.models[model_name]
            self.best_model_name = model_name
            print(f"\n‚úì M√¥ h√¨nh t·ªët nh·∫•t ƒë∆∞·ª£c ch·ªçn: {model_name}")
        else:
            print(f"‚ùå M√¥ h√¨nh '{model_name}' kh√¥ng t·ªìn t·∫°i!")


def main():
    """Test model training"""
    print("Module Models Training ƒë√£ s·∫µn s√†ng!")
    print("S·ª≠ d·ª•ng: from src.models import DiabetesModelTrainer")


if __name__ == "__main__":
    main()
